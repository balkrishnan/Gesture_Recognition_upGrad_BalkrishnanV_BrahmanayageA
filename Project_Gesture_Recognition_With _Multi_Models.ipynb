{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started. Once you have completed the code you can download the notebook for making a submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Gesture Recognition Project\n",
    "==========================\n",
    "This notebook provides a streamlined approach to developing gesture recognition models\n",
    "using three different architectures and optional transfer learning components.\n",
    "\n",
    "Supported model architectures:\n",
    "- Conv3D\n",
    "- Conv2D + LSTM\n",
    "- Conv2D + GRU\n",
    "\n",
    "Dataset path: /home/datasets/Project_data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from imageio import imread\n",
    "from skimage.transform import resize # For image resize; as per assignment images of different sizes will need be resized to similiar size before trainig\n",
    "import datetime\n",
    "import random\n",
    "import matplotlib.pyplot as plt # For Visualisation\n",
    "from PIL import Image, ImageFilter, ImageEnhance # For Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow components\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv3D, MaxPooling3D, Conv2D, MaxPooling2D, TimeDistributed,\n",
    "    Dense, Dropout, Flatten, BatchNormalization, \n",
    "    GlobalAveragePooling2D, GlobalAveragePooling3D,\n",
    "    LSTM, GRU, Input, Add, Concatenate, LeakyReLU, Bidirectional\n",
    ")\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping # Callbacks\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam # Optimizer - Always ADAM\n",
    "from tensorflow.keras.applications import MobileNetV2, ResNet50V2 # for transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**data path: /home/datasets/Project_data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "\n",
    "\n",
    "def load_data(data_path='/home/datasets/Project_data'):\n",
    "    \"\"\"Load training and validation data from CSV files.\"\"\"\n",
    "    train_doc = np.random.permutation(open(f'{data_path}/train.csv').readlines())\n",
    "    val_doc = np.random.permutation(open(f'{data_path}/val.csv').readlines())\n",
    "    \n",
    "    print(f'# training sequences = {len(train_doc)}')\n",
    "    print(f'# validation sequences = {len(val_doc)}')\n",
    "    \n",
    "    return train_doc, val_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Class distribution\n",
    "\n",
    "**Need to see if it affects training**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_class_distribution(data_list):\n",
    "    \"\"\"Analyze class distribution and return class weights using median frequency balancing.\"\"\"\n",
    "    class_counts = {0:0, 1:0, 2:0, 3:0, 4:0}\n",
    "    \n",
    "    for entry in data_list:\n",
    "        label = int(entry.strip().split(';')[2])\n",
    "        class_counts[label] += 1\n",
    "    \n",
    "    total = sum(class_counts.values())\n",
    "    percentages = {c: (count/total*100) for c, count in class_counts.items()}\n",
    "    \n",
    "    print(\"Class distribution:\")\n",
    "    for cls, count in class_counts.items():\n",
    "        print(f\"Class {cls}: {count} samples ({percentages[cls]:.1f}%)\")\n",
    "    \n",
    "    # Using median frequency balancing\n",
    "    median = np.median(list(class_counts.values()))\n",
    "    class_weights = {c: median/count for c, count in class_counts.items()}\n",
    "    \n",
    "    print(\"\\nClass weights using median frequency balancing:\")\n",
    "    for c, w in class_weights.items():\n",
    "        print(f\"Class {c}: {w:.2f}\")\n",
    "    \n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dynamic frame selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_frame_selection(total_frames, num_frames=10):\n",
    "    \"\"\"\n",
    "    Dynamically select frames from a video of any length.\n",
    "    \n",
    "    Args:\n",
    "        total_frames: Total number of frames in the video\n",
    "        num_frames: Number of frames to select\n",
    "        \n",
    "    Returns:\n",
    "        List of frame indices\n",
    "    \"\"\"\n",
    "    if total_frames <= num_frames:\n",
    "        # If we have fewer frames than needed, may need to repeat\n",
    "        indices = list(range(total_frames))\n",
    "        # If needed, add  sampling from existing frames\n",
    "        if len(indices) < num_frames:\n",
    "            extra = random.choices(indices, k=num_frames-len(indices))\n",
    "            indices.extend(extra)\n",
    "        return sorted(indices)\n",
    "    else:\n",
    "        # If we have more frames than needed, sample evenly across the video\n",
    "        return sorted(np.linspace(0, total_frames - 1, num_frames).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with sample sequence\n",
    "**Square Cropping**\n",
    "**Resizing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample_sequence(source_path, folder_str, seq_length=10, dim_x=120, dim_y=120):\n",
    "    \"\"\"Visualize a sample sequence of images from the dataset.\"\"\"\n",
    "    folder_name = folder_str.strip().split(';')[0]\n",
    "    label = int(folder_str.strip().split(';')[2])\n",
    "    \n",
    "    folder_path = os.path.join(source_path, folder_name)\n",
    "    imgs = sorted([f for f in os.listdir(folder_path) \n",
    "                if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    \n",
    "    # Use dynamic frame selection\n",
    "    selected_indices = dynamic_frame_selection(len(imgs), seq_length)\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        if idx < len(imgs):\n",
    "            image_path = os.path.join(folder_path, imgs[idx])\n",
    "            image = imread(image_path)\n",
    "            \n",
    "            # Display original image\n",
    "            plt.subplot(2, len(selected_indices), i+1)\n",
    "            plt.title(f\"Original #{idx}\")\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Process image with square crop\n",
    "            h, w = image.shape[:2]\n",
    "            crop_size = min(h, w)\n",
    "            y = (h - crop_size) // 2\n",
    "            x = (w - crop_size) // 2\n",
    "            cropped = image[y:y+crop_size, x:x+crop_size]\n",
    "            \n",
    "            # Resize to target dimensions\n",
    "            resized = resize(cropped, (dim_x, dim_y))\n",
    "            \n",
    "            # Display processed image\n",
    "            plt.subplot(2, len(selected_indices), i+len(selected_indices)+1)\n",
    "            plt.title(f\"Processed #{idx}\")\n",
    "            plt.imshow(resized)\n",
    "            plt.axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Sample sequence from class {label}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy.\n",
    "\n",
    "**Generates batches of video sequences for training or validation**\n",
    "\n",
    "***Dynamically selects frames for each sequence.***\n",
    "\n",
    "***Applies preprocessing (cropping, resizing) and optional augmentations (rotation, flipping, brightness adjustment).***\n",
    "\n",
    "***Normalizes images based on the model type:***\n",
    "\n",
    "> ***For MobileNetV2 or ResNet50V2, scales pixel values to [−1,1][−1,1].***\n",
    "\n",
    "> ***Otherwise, normalizes to [1][1].***\n",
    "\n",
    "***Outputs batches in the format (batch_size,seq_length,height,width,channels)(batch_size,seq_length,height,width,channels).***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DATA GENERATOR\n",
    "\n",
    "\n",
    "def generator(source_path, folder_list, batch_size, seq_length=10,\n",
    "              dim_x=120, dim_y=120, is_train=False, enable_augmentation=False, model_type=None,base_model=None):\n",
    "    \"\"\"\n",
    "    Simplified generator with motion-aware augmentation and proper normalization.\n",
    "    \n",
    "    Args:\n",
    "        source_path: Path to the source directory\n",
    "        folder_list: List of folders to process\n",
    "        batch_size: Number of sequences per batch\n",
    "        seq_length: Number of frames to use for each sequence\n",
    "        dim_x, dim_y: Target dimensions for the images\n",
    "        is_train: Whether this generator is for training data\n",
    "        enable_augmentation: Whether to apply data augmentation\n",
    "        model_type: Type of model to normalize for ('transfer' or None)\n",
    "        base_model: Base model in case of transfer learning\n",
    "    \"\"\"\n",
    "    print(f'Source path = {source_path}; batch size = {batch_size}; augmentation = {enable_augmentation}')\n",
    "    \n",
    "    while True:\n",
    "        # Shuffle the data for each epoch\n",
    "        if is_train:\n",
    "            t = np.random.permutation(folder_list)\n",
    "        else:\n",
    "            t = folder_list\n",
    "        \n",
    "        for batch_start in range(0, len(t), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(t))\n",
    "            current_batch = t[batch_start:batch_end]\n",
    "            current_batch_size = len(current_batch)\n",
    "            \n",
    "            # Initialize batch arrays\n",
    "            batch_data = np.zeros((batch_size, seq_length, dim_x, dim_y, 3), dtype=np.float32)\n",
    "            batch_labels = np.zeros((batch_size, 5))\n",
    "            \n",
    "            for folder_idx in range(current_batch_size):\n",
    "                folder_str = current_batch[folder_idx]\n",
    "                folder_name = folder_str.strip().split(';')[0]\n",
    "                label = int(folder_str.strip().split(';')[2])\n",
    "                \n",
    "                # Get the list of image files\n",
    "                folder_path = os.path.join(source_path, folder_name)\n",
    "                imgs = sorted([f for f in os.listdir(folder_path) \n",
    "                             if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "                \n",
    "                # Dynamic frame selection\n",
    "                selected_indices = dynamic_frame_selection(len(imgs), seq_length)\n",
    "                \n",
    "                # Generate sequence-level augmentation parameters (for temporal coherence)\n",
    "                sequence_aug_params = None\n",
    "                if is_train and enable_augmentation and random.random() < 0.5:\n",
    "                    sequence_aug_params = {\n",
    "                        # augmentation intensity\n",
    "                        'angle': random.uniform(-10, 10),\n",
    "                        'flip': random.random() < 0.5,\n",
    "                        'brightness': random.uniform(0.8, 1.2),\n",
    "                        'aug_type': random.randint(0, 4)\n",
    "                    }\n",
    "                \n",
    "                # Process each frame in the sequence\n",
    "                for frame_idx, img_pos in enumerate(selected_indices):\n",
    "                    if img_pos >= len(imgs):\n",
    "                        img_pos = len(imgs) - 1\n",
    "                    \n",
    "                    # Read the image\n",
    "                    image_path = os.path.join(folder_path, imgs[img_pos])\n",
    "                    image = imread(image_path)\n",
    "                    \n",
    "                    # Center crop to square\n",
    "                    h, w = image.shape[:2]\n",
    "                    crop_size = min(h, w)\n",
    "                    y = (h - crop_size) // 2\n",
    "                    x = (w - crop_size) // 2\n",
    "                    cropped = image[y:y+crop_size, x:x+crop_size]\n",
    "                    \n",
    "                    # Resize to target dimensions\n",
    "                    resized = resize(cropped, (dim_x, dim_y))\n",
    "                    \n",
    "                    # Convert to uint8 for PIL processing (0-255 range)\n",
    "                    image_uint8 = (resized * 255).astype(np.uint8)\n",
    "                    \n",
    "                    # Apply consistent sequence-level augmentations if enabled\n",
    "                    if sequence_aug_params is not None:\n",
    "                        # Convert to PIL image for augmentation\n",
    "                        pil_image = Image.fromarray(image_uint8)\n",
    "                        \n",
    "                        # Apply rotation \n",
    "                        if sequence_aug_params['angle'] != 0:\n",
    "                            pil_image = pil_image.rotate(\n",
    "                                sequence_aug_params['angle'], \n",
    "                                resample=Image.BILINEAR,\n",
    "                                expand=False\n",
    "                            )\n",
    "                        \n",
    "                        # Apply flip\n",
    "                        if sequence_aug_params['flip']:\n",
    "                            pil_image = pil_image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                        \n",
    "                        # Apply filter-based augmentations\n",
    "                        aug_type = sequence_aug_params['aug_type']\n",
    "                        if aug_type == 0:  # Edge Enhancement\n",
    "                            pil_image = pil_image.filter(ImageFilter.EDGE_ENHANCE)\n",
    "                        elif aug_type == 1:  # Gaussian Blur (reduced intensity)\n",
    "                            pil_image = pil_image.filter(ImageFilter.GaussianBlur(0.5))\n",
    "                        elif aug_type == 2:  # Detail enhancement\n",
    "                            pil_image = pil_image.filter(ImageFilter.DETAIL)\n",
    "                        elif aug_type == 3:  # Sharpening\n",
    "                            pil_image = pil_image.filter(ImageFilter.SHARPEN)\n",
    "                        elif aug_type == 4:  # Brightness adjustment\n",
    "                            enhancer = ImageEnhance.Brightness(pil_image)\n",
    "                            pil_image = enhancer.enhance(sequence_aug_params['brightness'])\n",
    "                        \n",
    "                        # Convert back to numpy array and normalize to [0, 1]\n",
    "                        resized = np.array(pil_image).astype(np.float32) / 255.0\n",
    "                    else:\n",
    "                        # Normalize original resized image to [0, 1]\n",
    "                        resized = resized.astype(np.float32)\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    if model_type == 'transfer':\n",
    "                        # Apply simple normalization based on the model type\n",
    "                        if base_model and 'mobilenet' in base_model.lower():\n",
    "                            # MobileNetV2: scales to [-1,1]\n",
    "                            resized = (resized * 2.0) - 1.0\n",
    "                        elif base_model and 'resnet' in base_model.lower():\n",
    "                            # ResNet: simple normalization to [-1,1]\n",
    "                            resized = (resized * 2.0) - 1.0\n",
    "                    else:\n",
    "                        # Default normalization to [-1,1]\n",
    "                        resized = (resized - 0.5) * 2.0\n",
    "                    \n",
    "                    # Store in batch\n",
    "                    batch_data[folder_idx, frame_idx] = resized\n",
    "                \n",
    "                # One-hot encode the label\n",
    "                batch_labels[folder_idx, label] = 1\n",
    "            \n",
    "            # If batch is not full (last batch might be smaller), slice to actual size\n",
    "            if current_batch_size < batch_size:\n",
    "                batch_data = batch_data[:current_batch_size]\n",
    "                batch_labels = batch_labels[:current_batch_size]\n",
    "            \n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam.\n",
    "\n",
    "a. Conv3D Model - Residual Connection, Batchnormalization , Leaky Relu, Dropuout ends with soft max\n",
    "b. CNN-GRU Model - Timeddistributed CNN with temporal model with GRU\n",
    "c. CNN-LSTM Model - Similiar to (b) but with LSTM\n",
    "d. Transfer Learning Model - Uses pre-trained MobileNetV2 or ResNet50V2; fine-tune top layer with unfreeze percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MODEL ARCHITECTURES\n",
    "\n",
    "\n",
    "def create_conv3d_model(input_shape, learning_rate=0.0001):\n",
    "    \"\"\"3D CNN model with residual connections.\n",
    "    \n",
    "       \n",
    "    Args:\n",
    "        input_shape (tuple): Shape of the input data (frames, height, width, channels).\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "    \n",
    "    Returns:\n",
    "        model: Compiled Keras model.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # First block\n",
    "    x = Conv3D(16, kernel_size=(3, 3, 3), padding='same')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPooling3D(pool_size=(1, 2, 2))(x)\n",
    "    \n",
    "    # Second block with residual connection\n",
    "    res = Conv3D(32, kernel_size=(1, 1, 1), padding='same')(x)\n",
    "    x = Conv3D(32, kernel_size=(3, 3, 3), padding='same')(x)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = Add()([x, res])  # Residual connection\n",
    "    x = MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Third block with residual connection\n",
    "    res = Conv3D(64, kernel_size=(1, 1, 1), padding='same')(x)\n",
    "    x = Conv3D(64, kernel_size=(3, 3, 3), padding='same')(x)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = Add()([x, res])  # Residual connection\n",
    "    x = GlobalAveragePooling3D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(5, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['categorical_accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def create_cnn_gru_model(input_shape, learning_rate=0.0001):\n",
    "    \"\"\"\n",
    "       \n",
    "    Args:\n",
    "        input_shape (tuple): Shape of the input data (frames, height, width, channels).\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "    \n",
    "    Returns:\n",
    "        model: Compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # TimeDistributed CNN layers\n",
    "        TimeDistributed(Conv2D(16, (3, 3), padding='same'), input_shape=input_shape),\n",
    "        TimeDistributed(BatchNormalization(momentum=0.9)),\n",
    "        TimeDistributed(LeakyReLU(alpha=0.1)),\n",
    "        TimeDistributed(MaxPooling2D((2, 2))),\n",
    "        \n",
    "        TimeDistributed(Conv2D(32, (3, 3), padding='same')),\n",
    "        TimeDistributed(BatchNormalization(momentum=0.9)),\n",
    "        TimeDistributed(LeakyReLU(alpha=0.1)),\n",
    "        TimeDistributed(MaxPooling2D((2, 2))),\n",
    "        \n",
    "        # Temporal GRU layers\n",
    "        TimeDistributed(Flatten()),\n",
    "        GRU(128, return_sequences=True),\n",
    "        GRU(64),\n",
    "        \n",
    "        # Classification\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['categorical_accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_cnn_lstm_model(input_shape, learning_rate=0.0001):\n",
    "    \"\"\"\n",
    "       \n",
    "    Args:\n",
    "        input_shape (tuple): Shape of the input data (frames, height, width, channels).\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "    \n",
    "    Returns:\n",
    "        model: Compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # TimeDistributed CNN layers\n",
    "        TimeDistributed(Conv2D(16, (3, 3), activation='relu', padding='same'), input_shape=input_shape),\n",
    "        TimeDistributed(MaxPooling2D((2, 2))),\n",
    "        \n",
    "        TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same')),\n",
    "        TimeDistributed(MaxPooling2D((2, 2))),\n",
    "        \n",
    "        # Flatten and LSTM layers\n",
    "        TimeDistributed(Flatten()),\n",
    "        LSTM(64, return_sequences=True),\n",
    "        LSTM(64),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(5, activation='softmax')  # Assuming 5 classes for classification\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['categorical_accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_transfer_learning_model(input_shape, learning_rate=0.0001, base_model='mobilenet', unfreeze_percent=30):\n",
    "    \"\"\"\n",
    "    Create a transfer learning model with pretrained CNN and temporal processing.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Input shape (frames, height, width, channels)\n",
    "        learning_rate: Learning rate for the optimizer\n",
    "        base_model: Base model to use ('mobilenet' or 'resnet')\n",
    "        unfreeze_percent: Percentage of layers to unfreeze from the top (e.g., 30 means unfreeze the last 30% of layers)\n",
    "    \"\"\"\n",
    "    # Get number of frames and image dimensions\n",
    "    frames, height, width, channels = input_shape\n",
    "    \n",
    "    # Choose base model\n",
    "    if base_model.lower() == 'mobilenet':\n",
    "        base_cnn = MobileNetV2(\n",
    "            input_shape=(height, width, channels),\n",
    "            include_top=False,\n",
    "            weights='imagenet'\n",
    "        )\n",
    "    elif base_model.lower() == 'resnet':\n",
    "        base_cnn = ResNet50V2(\n",
    "            input_shape=(height, width, channels),\n",
    "            include_top=False,\n",
    "            weights='imagenet'\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported base model: {base_model}\")\n",
    "    \n",
    "    # Calculate number of layers to unfreeze\n",
    "    total_layers = len(base_cnn.layers)\n",
    "    unfreeze_layers = max(1, int(total_layers * unfreeze_percent / 100))\n",
    "    print(f\"Total layers in {base_model}: {total_layers}\")\n",
    "    print(f\"Unfreezing the last {unfreeze_layers} layers ({unfreeze_percent}% of total)\")\n",
    "    \n",
    "    # Freeze most layers but unfreeze the specified percentage from the top\n",
    "    for layer in base_cnn.layers[:-unfreeze_layers]:\n",
    "        layer.trainable = False\n",
    "    for layer in base_cnn.layers[-unfreeze_layers:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # Create feature extractor\n",
    "    feature_extractor = Model(\n",
    "        inputs=base_cnn.input,\n",
    "        outputs=GlobalAveragePooling2D()(base_cnn.output)\n",
    "    )\n",
    "    \n",
    "    # Build full model\n",
    "    sequence_input = Input(shape=input_shape)\n",
    "    encoded_frames = TimeDistributed(feature_extractor)(sequence_input)\n",
    "    \n",
    "    # Temporal processing\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(encoded_frames)\n",
    "    x = LSTM(64, return_sequences=False)(x)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    # Classification\n",
    "    outputs = Dense(5, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=sequence_input, outputs=outputs)\n",
    "    \n",
    "    # Lower learning rate for transfer learning\n",
    "    optimizer = Adam(learning_rate=learning_rate / 5)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['categorical_accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility fucntions for all models\n",
    "**Callbacks**\n",
    "**Plotting**\n",
    "**Step Calculation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TRAINING UTILITIES\n",
    "\n",
    "\n",
    "def create_callbacks(model_name_prefix):\n",
    "    \"\"\"\n",
    "\n",
    "    ModelCheckpoint: Saves the best model based on validation accuracy.\n",
    "\n",
    "    ReduceLROnPlateau: Reduces learning rate if validation loss plateaus.\n",
    "\n",
    "    EarlyStopping: Stops training if validation loss does not improve for several epochs.\n",
    ".\"\"\"\n",
    "    # Generate model name with timestamp\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    model_name = f\"{model_name_prefix}_{timestamp}\"\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "    \n",
    "    # Define checkpoint filepath\n",
    "    filepath = os.path.join(\n",
    "        model_name, \n",
    "        \"model-{epoch:03d}-{val_categorical_accuracy:.4f}.h5\"\n",
    "    )\n",
    "    \n",
    "    # Model checkpoint callback\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=filepath,\n",
    "        monitor='val_categorical_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Reduce learning rate callback\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return [checkpoint, reduce_lr, early_stopping]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main training function\n",
    "**Checks class imbalance**\n",
    "** Visualize sample**\n",
    "** Sets up data geenrators**\n",
    "** Creates models**\n",
    "** Compile models**\n",
    "** Calculate steps**\n",
    "** Defines Callbacks**\n",
    "** Trains Model**\n",
    "** Plots Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAIN TRAINING FUNCTION\n",
    "\n",
    "\n",
    "def train_gesture_model(model_type='conv3d', transfer_learning=False, base_model='resnet',\n",
    "                        batch_size=32, num_epochs=50, enable_augmentation=True,\n",
    "                        learning_rate=0.0001, dim_x=120, dim_y=120, seq_length=10,\n",
    "                        unfreeze_percent=30):\n",
    "    \"\"\"\n",
    "    Train a gesture recognition model with the specified configuration.\n",
    "    \n",
    "    Args:\n",
    "        model_type: Type of model to train ('conv3d', 'cnn_lstm', 'cnn_gru', or 'transfer')\n",
    "        transfer_learning: Whether to use transfer learning\n",
    "        base_model: Base model for transfer learning ('mobilenet' or 'resnet')\n",
    "        batch_size: Batch size for training\n",
    "        num_epochs: Number of epochs to train\n",
    "        enable_augmentation: Whether to use data augmentation\n",
    "        learning_rate: Learning rate for the optimizer\n",
    "        dim_x, dim_y: Target dimensions for the images\n",
    "        seq_length: Number of frames to use for each sequence\n",
    "        unfreeze_percent: Percentage of layers to unfreeze from the top (for transfer learning)\n",
    "    \"\"\"\n",
    "    # Load and validate datasets\n",
    "    data_path = '/home/datasets/Project_data'\n",
    "    train_path = f'{data_path}/train'\n",
    "    val_path = f'{data_path}/val'\n",
    "    \n",
    "    train_doc, val_doc = load_data(data_path)\n",
    "    \n",
    "    # Analyze class distribution and get class weights\n",
    "    class_weights = analyze_class_distribution(train_doc)\n",
    "    \n",
    "    # Visualize a sample sequence\n",
    "    visualize_sample_sequence(train_path, train_doc[0], seq_length, dim_x, dim_y)\n",
    "    \n",
    "    # Determine generator normalization based on model type\n",
    "    generator_model_type = 'transfer' if transfer_learning or model_type == 'transfer' else None\n",
    "    \n",
    "    # Set up data generators\n",
    "    train_generator = generator(\n",
    "        train_path, train_doc, batch_size, seq_length,\n",
    "        dim_x, dim_y, is_train=True, enable_augmentation=enable_augmentation,\n",
    "        model_type=generator_model_type,base_model=base_model\n",
    "    )\n",
    "    \n",
    "    # No augmentation for validation\n",
    "    val_generator = generator(\n",
    "        val_path, val_doc, batch_size, seq_length,\n",
    "        dim_x, dim_y, is_train=False, enable_augmentation=False,\n",
    "        model_type=generator_model_type,base_model=base_model\n",
    "    )\n",
    "    \n",
    "    # Calculate steps per epoch\n",
    "    steps_per_epoch = (len(train_doc) + batch_size - 1) // batch_size\n",
    "    validation_steps = (len(val_doc) + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f'Steps per epoch: {steps_per_epoch}')\n",
    "    print(f'Validation steps: {validation_steps}')\n",
    "    \n",
    "    # Define input shape - (frames, height, width, channels)\n",
    "    input_shape = (seq_length, dim_x, dim_y, 3)\n",
    "    \n",
    "    # Create model\n",
    "    if transfer_learning or model_type == 'transfer':\n",
    "        model = create_transfer_learning_model(\n",
    "            input_shape, learning_rate, base_model, unfreeze_percent\n",
    "        )\n",
    "        model_name_prefix = f'gesture_transfer_{base_model}'\n",
    "    elif model_type == 'conv3d':\n",
    "        model = create_conv3d_model(input_shape, learning_rate)\n",
    "        model_name_prefix = 'gesture_conv3d'\n",
    "    elif model_type == 'cnn_lstm':\n",
    "        model = create_cnn_lstm_model(input_shape, learning_rate)\n",
    "        model_name_prefix = 'gesture_cnn_lstm'\n",
    "    elif model_type == 'cnn_gru':\n",
    "        model = create_cnn_gru_model(input_shape, learning_rate)\n",
    "        model_name_prefix = 'gesture_cnn_gru'\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    # Display model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Create callbacks\n",
    "    callbacks = create_callbacks(model_name_prefix)\n",
    "    \n",
    "    # Train model with class weights\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Save final model\n",
    "    model.save(f\"{model_name_prefix}_final.h5\")\n",
    "    \n",
    "    return model, history, model_name_prefix\n",
    "\n",
    "def plot_training_validation_accuracy(results):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Define different line styles and colors for better visualization\n",
    "    colors = ['blue', 'red', 'green', 'purple', 'orange']\n",
    "    styles = ['-', '--', '-.', ':']\n",
    "    \n",
    "    for i, (model_name, result) in enumerate(results.items()):\n",
    "        history = result['history']  # This is correct - we access the history object\n",
    "        epochs = range(1, len(history.history['categorical_accuracy']) + 1)\n",
    "        \n",
    "        color = colors[i % len(colors)]\n",
    "        \n",
    "        # Plot training accuracy\n",
    "        plt.plot(epochs, history.history['categorical_accuracy'], \n",
    "                 color=color, linestyle=styles[0],\n",
    "                 label=f'{model_name} - Training')\n",
    "        \n",
    "        # Plot validation accuracy\n",
    "        plt.plot(epochs, history.history['val_categorical_accuracy'], \n",
    "                 color=color, linestyle=styles[1], \n",
    "                 label=f'{model_name} - Validation')\n",
    "    \n",
    "    # Graph details\n",
    "    plt.title('Training and Validation Accuracy for Each Model')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig('model_comparison_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing it all together\n",
    "**Training all models sequentially, save, and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3765258053.py, line 71)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_194/3765258053.py\"\u001b[0;36m, line \u001b[0;32m71\u001b[0m\n\u001b[0;31m    \"best_train_acc\" = max(history.history['categorical_accuracy'])\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# MAIN EXECUTION\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    SEQ_LENGTH = 10          # Number of frames per sequence\n",
    "    DIM_X, DIM_Y = 120, 120  # Image dimensions\n",
    "    LEARNING_RATE = 0.0001   # Learning rate\n",
    "    BATCH_SIZE = 32          # Batch size for standard models\n",
    "    TL_BATCH_SIZE = 16       # Reduced batch size for transfer learning because of heavy memory consumption\n",
    "    NUM_EPOCHS = 50          # Number of epochs\n",
    "    \n",
    "    print(\"Gesture Recognition Training\")\n",
    "    print(f\"- Frames per sequence: {SEQ_LENGTH}\")\n",
    "    print(f\"- Image dimensions: {DIM_X}x{DIM_Y}\")\n",
    "    print(f\"- Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"- Batch size (standard models): {BATCH_SIZE}\")\n",
    "    print(f\"- Batch size (transfer learning): {TL_BATCH_SIZE}\")\n",
    "    print(f\"- Max epochs: {NUM_EPOCHS}\")\n",
    "    \n",
    "    # Models to train\n",
    "    models = [\n",
    "        {\"type\": \"conv3d\", \"name\": \"3D CNN\", \"batch_size\": BATCH_SIZE},\n",
    "        {\"type\": \"cnn_lstm\", \"name\": \"CNN+LSTM\", \"batch_size\": BATCH_SIZE},\n",
    "        {\"type\": \"cnn_gru\", \"name\": \"CNN+GRU\", \"batch_size\": BATCH_SIZE},\n",
    "        {\"type\": \"transfer\", \"name\": \"MobileNet+LSTM\", \"base\": \"mobilenet\", \n",
    "         \"batch_size\": TL_BATCH_SIZE, \"unfreeze_percent\": 30},\n",
    "        {\"type\": \"transfer\", \"name\": \"ResNet+LSTM\", \"base\": \"resnet\", \n",
    "         \"batch_size\": TL_BATCH_SIZE, \"unfreeze_percent\": 20}\n",
    "    ]\n",
    "    \n",
    "    # Train each model\n",
    "    results = {}\n",
    "    for model_config in models:\n",
    "        print(f\"\\n{'-'*50}\")\n",
    "        print(f\"Training {model_config['name']} model\")\n",
    "        print(f\"{'-'*50}\")\n",
    "        \n",
    "        # Clear any previous model from memory\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        # Set parameters\n",
    "        params = {\n",
    "            \"model_type\": model_config[\"type\"],\n",
    "            \"batch_size\": model_config[\"batch_size\"],\n",
    "            \"num_epochs\": NUM_EPOCHS,\n",
    "            \"enable_augmentation\": True,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"dim_x\": DIM_X,\n",
    "            \"dim_y\": DIM_Y,\n",
    "            \"seq_length\": SEQ_LENGTH\n",
    "        }\n",
    "        \n",
    "        # Add transfer learning parameters if applicable\n",
    "        if \"base\" in model_config:\n",
    "            params[\"transfer_learning\"] = True\n",
    "            params[\"base_model\"] = model_config[\"base\"]\n",
    "            \n",
    "            # Add unfreeze percentage if specified\n",
    "            if \"unfreeze_percent\" in model_config:\n",
    "                params[\"unfreeze_percent\"] = model_config[\"unfreeze_percent\"]\n",
    "        \n",
    "        # Train the model\n",
    "        model, history, prefix = train_gesture_model(**params)\n",
    "        \n",
    "        # Store results\n",
    "        results[model_config[\"name\"]] = {\n",
    "            \"model\": model,\n",
    "            \"history\": history,\n",
    "            \"best_val_acc\": max(history.history['val_categorical_accuracy']),\n",
    "            \"best_train_acc\": max(history.history['categorical_accuracy'])\n",
    "        }\n",
    "        \n",
    "        # Clear memory\n",
    "        tf.keras.backend.clear_session()\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nTraining Results:\")\n",
    "    print(f\"{'Model':<20} {'Best Validation Accuracy':<25}\")\n",
    "    print(\"-\" * 45)\n",
    "    for name, result in results.items():\n",
    "        print(f\"{name:<20} {result['best_val_acc']:.4f}\")\n",
    "        \n",
    "    # Find the best model\n",
    "    best_model_name = max(results.keys(), key=lambda k: results[k]['best_val_acc'])\n",
    "    best_acc = results[best_model_name]['best_val_acc']\n",
    "    \n",
    "    print(f\"\\nBest performing model: {best_model_name} with validation accuracy of {best_acc:.4f}\")\n",
    "    print(\"All models have been saved to disk with prefix 'gesture_*_final.h5'\")\n",
    "    plot_training_validation_accuracy(results)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
